{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AkuPusingMas",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbcOJNmCB1CK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2af7e1d8-c367-4502-e411-271067581246"
      },
      "source": [
        "from google.colab import drive; drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYbue8r-ptUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0b6d888-d1e3-42fe-f400-2af1f1db7429"
      },
      "source": [
        "#pip install --upgrade tensorflow-gpu\n",
        "#!pip install Keras==2.2.4\n",
        "!pip install tokenizers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.8.1rc1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nekFGFpBY4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66dfe720-f763-458f-f27d-7347e5be9faf"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import sentencepiece as spm\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "print(\"Tf\",tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tf 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VWe8Nd_8ee2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "0ae4f927-e623-42dc-aba1-b0afe2a4c4e7"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaFTp1T5UG4u",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJE6XonlC0cP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HXeENPACdrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  from transformers import XLMRobertaTokenizer\n",
        "  from transformers import XLMRobertaModel"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6U3PIasB0ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 96  \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "PATH = \"/content/drive/My Drive/Gemastik/XLMRoberta\"\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"jplu/tf-xlm-roberta-large\",add_prefix_space = True,lowercase = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE0vwUt3Bve_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " traindf = pd.read_csv(\"/content/drive/My Drive/Gemastik/PreprocessedManual.csv\")\n",
        " traindf.drop([\"Unnamed: 0\"],axis = 1,inplace = True)\n",
        " #traindf = pd.read_csv(\"/content/drive/My Drive/Gemastik/DatasetLabeled.csv\",delimiter = \"\\t\")\n",
        " #traindf.head()\n",
        " #traindf5000 = traindf[:5000]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsgOv-fuoaQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        truncation=True,  \n",
        "        return_token_type_ids=True,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbgFKttZvxo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fbdd023-1906-4756-da2b-acda7667a10c"
      },
      "source": [
        "x_train,x_val,y_train,y_val = train_test_split(traindf[\"full_text\"],traindf[\"is_accident\"],\n",
        "                                               test_size = 0.1, random_state = 0)\n",
        "x_train = regular_encode(x_train,tokenizer,MAX_LEN)\n",
        "x_val = regular_encode(x_val,tokenizer,MAX_LEN)\n",
        "print(x_train.shape,x_val.shape,y_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(901, 96) (101, 96) (901,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56LONE-D1ZVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KRDxojEviDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_train = fast_encode(X_train.astype(str),fast_tokenizer,chunk_size=256,maxlen=96)\n",
        "#x_valid = fast_encode(X_valid.astype(str),fast_tokenizer,chunk_size=256,maxlen=96)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L9-4yPq14Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(64)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkaEADAY14sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_val, y_val))\n",
        "    .batch(64)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J4wnIbXlJWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "5c0cfa48-07c4-48d8-8099-4f60a38cacfd"
      },
      "source": [
        "import keras\n",
        "def main_model():\n",
        "  config = XLMRobertaConfig.from_pretrained(\"jplu/tf-xlm-roberta-large\")\n",
        "  config.num_labels = 2\n",
        "  config.use_bfloat16 = 'use_fp16'\n",
        "  config.start_tok = \"<s>\",\n",
        "  config.end_tok = \"</s>\",  \n",
        "  ENCODER = TFXLMRobertaModel.from_pretrained(\"jplu/tf-xlm-roberta-large\",config = config)\n",
        "  input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "  attention_mask = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "  embedding = ENCODER(input_ids)[0]\n",
        "  #Disini Boleh Di adjust\n",
        "  #logits = tf.keras.layers.GlobalAveragePooling1D()(embedding)\n",
        "  logits = tf.keras.layers.Dropout(0.1)(embedding)\n",
        "  logits = tf.keras.layers.Flatten()(logits)\n",
        "  logits = tf.keras.layers.Dense(units = 1024,activation = \"tanh\")(logits)\n",
        "  logits = tf.keras.layers.Dropout(0.60)(logits)\n",
        "  logits = tf.keras.layers.Dense(16,activation=\"tanh\")(logits)\n",
        "  out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"final_output\")(logits)\n",
        "  model = tf.keras.Model(inputs=[input_ids], outputs=out)\n",
        "\n",
        "  loss = tf.keras.losses.BinaryCrossentropy()\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "  metrics=['accuracy']\n",
        "\n",
        "  for layer in model.layers[:2]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "  return model\n",
        "\n",
        "model = main_model()\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at jplu/tf-xlm-roberta-large were not used when initializing TFXLMRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFXLMRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-large.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 96)]              0         \n",
            "_________________________________________________________________\n",
            "tfxlm_roberta_model_4 (TFXLM ((None, 96, 1024), (None, 559890432 \n",
            "_________________________________________________________________\n",
            "dropout_375 (Dropout)        (None, 96, 1024)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 98304)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              100664320 \n",
            "_________________________________________________________________\n",
            "dropout_376 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                16400     \n",
            "_________________________________________________________________\n",
            "final_output (Dense)         (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 660,571,169\n",
            "Trainable params: 100,680,737\n",
            "Non-trainable params: 559,890,432\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFFs-rZ0NGop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSlBkpO-2NRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''def build_model(transformer, max_len=512):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    input_word_ids = keras.layers.Input((MAX_LEN,), dtype='int32', name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_out = keras.layers.Dropout(0.5)(sequence_output)\n",
        "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
        "    logits = keras.layers.Dropout(0.5)(logits)\n",
        "    logits = keras.layers.Dense(units=1, activation=\"sigmoid\")(logits)\n",
        "    model = keras.Model(inputs=input_word_ids, outputs=logits)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRkLEplXuaaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''def build_model(transformer, max_len=512):\n",
        "  \n",
        "    input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4QZ6D2SNYxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''strategy = tf.distribute.get_strategy()\n",
        "with strategy.scope():\n",
        "        model = build_model(TFXLMRobertaModel.from_pretrained(\"jplu/tf-xlm-roberta-base\",num_labels = 2),MAX_LEN)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E3BYzut0rN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZvNLz062YqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#config=XLMRobertaConfig.from_pretrained(\"jplu/tf-xlm-roberta-base\")\n",
        "#model = build_model(TFXLMRobertaModel.from_pretrained(\"jplu/tf-xlm-roberta-base\",config= config), max_len=MAX_LEN)\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yHq2ftYq2HR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piTWc1BuTpAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjlJXqYUNFk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzAgHu-7KSN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
        "                                   end_learn_rate=1e-7,\n",
        "                                   warmup_epoch_count=10,\n",
        "                                   total_epoch_count=90):\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        if epoch < warmup_epoch_count:\n",
        "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
        "        else:\n",
        "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
        "        return float(res)\n",
        "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "    return learning_rate_scheduler"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHvtOkir5PjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "4d69c2c3-274c-43b4-afde-3928b9b3fe16"
      },
      "source": [
        "total_epoch_count = 10\n",
        "n_steps = x_train.shape[0]\n",
        "model.fit(x = train_dataset,\n",
        "          shuffle=True,\n",
        "          steps_per_epoch=n_steps,\n",
        "          validation_data=valid_dataset,\n",
        "          epochs=total_epoch_count,\n",
        "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
        "                                                    end_learn_rate=1e-7,\n",
        "                                                    warmup_epoch_count=20,\n",
        "                                                    total_epoch_count=total_epoch_count),\n",
        "                     keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "                     ]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
            "Epoch 1/10\n",
            "636/901 [====================>.........] - ETA: 6:01 - loss: 0.6754 - accuracy: 0.6188"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvhi0VTcUeh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save_weights(model_save_location, overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}